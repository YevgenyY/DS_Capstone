---
title: 'Data Science Capstone Project Milestone Report'
author: "Yevgeny V. Yorkhov"
date: "March 16, 2016"
output: html_document
---

# Synopsis

This is a report of Coursera Data Science Capstone project assignment. Around the world people spend lot's of time on their mobile devices typing words, phrases and sentences. Making typing easier is a pretty good task for mobile developers. The cornerstone of this task is predictive text models. In the Capstone Project we work on understanding and developing such models.

The report shows the exploratory analysis of the data from a corpus called HC Corpora [www.corpora.heliohost.org](http://www.corpora.heliohost.org). 

# Dataset

Dataset is available at the [following URL](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip). It consist of texts downloaded from different Internet websites, divided by 4-languages (English, Russian, German, Finnish) and parted into three files according to the text's source:


- texts from blogs (i.e. en_US.blogs.txt)
- texts from news (i.e. en_US.news.txt)
- texts from twitter (i.e. en_US.twitter.txt)

Also the readme of the dataset is available [here](http://www.corpora.heliohost.org/aboutcorpus.html )

In spite of the texts are language-filtered, but they may contain foreign text. Also the texts may contain some offensive words or phrases that shouldn't be used in the predictive text modeling.

# Basic statistics

Before cleaning the data check basic statistics about full dataset. The table below show the summary:

| Filename             | Size | Number of lines | Number of words |
| -------------------- | ---- | --------------- | --------------- |
| en_US.blogs.txt      | 201M |  899288         | 38222304        |
| en_US.news.txt       | 197M |  1010242        | 35710849        |
| en_US.twitter.txt    | 160M |  2360148        | 30433509        |
| ru_RU.blogs.txt      | 112M |  337100         | 9434050         |
| ru_RU.news.txt       | 114M |  196360         | 9125006         |
| ru_RU.twitter.txt    | 101M |  881414         | 9084961         |


```{r, basicStats, cache=TRUE}
getBasicStats <- function(x) {
  bytesMB <- 1024 * 1024
  conn <- file(x,open="r")
  raw <- readLines(conn)
  close(conn)
  
  res <- file.info(x)$size / bytesMB
  res <- c(res, length(raw))
  res <- c(res, sum(sapply(gregexpr("\\W+", raw), length)))

  names(res) <- c("file size", "number of lines", "number of words")  

  return(res)
}
```

# Data preprocessing

The data should be cleaned before modeling. As far as we are going to predict words we should remove numbers, punctuation, URLs, profanity words and stopwords (that came from grammar) from texts. In addition we should think about quality of sampling, because the dataset is large enough.

```{r, preprocessing, cache=TRUE, warning=FALSE, message=FALSE}
library(tm)
getCorpus <- function(filename, sampleSize, profanityWords) {
  conn <- file(filename,open="r")
  
  lines <- iconv(readLines(conn), to = "utf-8")
  if (sampleSize != 0) {
    # Sample data with uniform distribution
    rowNums <- round(runif(sampleSize, min=1, max=length(lines)),0)
    raw <- c(lines[1])
    for(i in rowNums) {
      raw <- c(raw, lines[i])
    }
    rm(lines)
  } else
    raw <- lines
  close(conn)

  # remove punctuation, numbers and tolower the content
  raw <- gsub("[^[:alnum:][:space:]']", ' ', raw)
  raw <- gsub('[[:digit:]]+', ' ', raw)
  raw <- gsub('[[:punct:]]+', '', raw)
  raw <- tolower(raw)
  
  # make a corpus
  txt <- VectorSource(raw)
  rm(raw)
  txt.corpus <- Corpus(txt)
  rm(txt)
  
  # Clean the corpus
  txt.corpus <- tm_map(txt.corpus, removeWords, stopwords("english"))
  txt.corpus <- tm_map(txt.corpus, removeWords, stopwords("russian"))
  
  txt.corpus <- tm_map(txt.corpus, removeWords, profanityWords)
  txt.corpus <- tm_map(txt.corpus, stripWhitespace)
  
  return(txt.corpus)
}

setwd("~/Coursera/DS_Capstone/")
blog.en="data/final/en_US/en_US.blogs.txt"
news.en="data/final/en_US/en_US.news.txt"
twitter.en="data/final/en_US/en_US.twitter.txt"
blog.ru="data/final/ru_RU/ru_RU.blogs.txt"
news.ru="data/final/ru_RU/ru_RU.news.txt"
twitter.ru="data/final/ru_RU/ru_RU.twitter.txt"

profanityWords.en <- names(read.csv(url("http://www.bannedwordlist.com/lists/swearWords.csv")))
profanityWords.ru <- names(read.csv("data/profanity_russian.txt"))

txt.en.blog <- getCorpus(blog.en, 10000, profanityWords.en)
txt.en.news <- getCorpus(news.en, 10000, profanityWords.en)
txt.en.twit <- getCorpus(twitter.en, 10000, profanityWords.en)
txt.ru.blog <- getCorpus(blog.ru, 10000, profanityWords.ru)
txt.ru.news <- getCorpus(news.ru, 10000, profanityWords.ru)
txt.ru.twit <- getCorpus(twitter.ru, 10000, profanityWords.ru)

txt.en <- c(txt.en.blog, txt.en.news, txt.en.twit)
txt.ru <- c(txt.ru.blog, txt.ru.news, txt.ru.twit)

# free memory
rm(txt.en.blog, txt.en.news, txt.en.twit, txt.ru.blog, txt.ru.news, txt.ru.twit)
```

# Question #1 
## Some words are more frequent than others - what are the distribution of word frequencies?

The interesting finding of Russian words frequency analysis is that the word ***'это'*** should be included in Russian stopwords list in R. This word means ***this** in English and it isn't included into stopwards("russian") function in R. 

```{r, words_frequency, cache=TRUE, warning=FALSE, message=FALSE}
library(slam)
getFrequency <- function(x) {
  tdm <- TermDocumentMatrix(x)
  tdm.999 <- removeSparseTerms(tdm, sparse = 0.999)
  rm(tdm)
 
  freq <- sort(row_sums(tdm.999), decreasing = TRUE)
  return(freq)
}

calcCoverage <- function(x) {
  df <- data.frame(matrix(nrow=length(x),ncol=2))
  names(df) <- c("index", "value")
  df$index <- c(1:length(x))
  df$value <- cumsum(x)*100/sum(x)

  return(df)
}

freq.en <- getFrequency(txt.en)
freq.ru <- getFrequency(txt.ru)
par(mfrow=c(1,2))
hist(freq.en, breaks=1000, main="English words frequency\ndistribution", xlab="Words index", border="blue")
hist(freq.ru, breaks=1000, main="Russian words frequency\ndistribution", xlab="Words index", border="red")
```

Most common words are plotted.
```{r, most_common_words, cache=TRUE, warning=FALSE}
library(ggplot2)
terms.en <- data.frame(term=names(freq.en), freq=freq.en)
ggplot(head(terms.en, 10), aes(reorder(term, -freq), freq, fill=freq)) +
  geom_bar(stat="identity") +
  ggtitle("Most common words") + xlab("Word") + ylab("Frequency")

terms.ru <- data.frame(term=names(freq.ru), freq=freq.ru)

ggplot(head(terms.ru, 10), aes(reorder(term, -freq), freq, fill=freq)) +
  geom_bar(stat="identity") +
  ggtitle("Most common words") + xlab("Word") + ylab("Frequency")

```

As we can see the word ***'это'*** has really higher frequency than any other Russian word in the corpus. This word means ***'this'*** in English and should be included in Russian stopwords list in R. Let's remove this word and make words frequency again.

```{r, most_common_words_correction, cache=TRUE, warning=FALSE, message=FALSE}

txt.ru <- tm_map(txt.ru, removeWords, "это")
txt.en <- tm_map(txt.en, removeWords,  c("don", "didn", "can", "doesn", "isn", "wasn", "t", "dont", "u", "s", "c", "a", "p", "m"))
freq.ru <- getFrequency(txt.ru)
terms.ru <- data.frame(term=names(freq.ru), freq=freq.ru)
ggplot(head(terms.ru, 10), aes(reorder(term, -freq), freq, fill=freq)) +
  geom_bar(stat="identity") +
  ggtitle("Most common words") + xlab("Word") + ylab("Frequency")
```

# Q2. What are the frequencies of 2-grams and 3-grams in the dataset? 

Plot most common Bigrams and Trigrams.

```{r, n-grams_processing, cache=TRUE, warning=FALSE, message=FALSE}
library(RWeka)
library(ggplot2)

getNGramFrequency <- function(corpus, ngram) {
  df <- data.frame(text = sapply(corpus, as.character), stringsAsFactors = FALSE)
  delim <- " \\r\\n\\t.,;:\"()?!&+“”‘’'/"
  df_tmp <- NGramTokenizer(df, Weka_control(min=ngram, max=ngram, delimiters = delim))
  
  df_ngram <- data.frame(table(df_tmp))
  names(df_ngram) <- c("Ngram", "freq")
  df_ngram <- df_ngram[order(df_ngram$freq, decreasing = TRUE),]
  
  rm(df,df_tmp)
  return(df_ngram)
}

freq.bi.en <- getNGramFrequency(txt.en, 2)
freq.tri.en <- getNGramFrequency(txt.en, 3)
freq.bi.ru <- getNGramFrequency(txt.ru, 2)
freq.tri.ru <- getNGramFrequency(txt.ru, 3)

ggplot(head(freq.bi.en, 10), aes(reorder(Ngram, -freq), freq, fill=freq)) +
  geom_bar(stat="identity") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggtitle("Most common EN bigrams") + xlab("Word") + ylab("Frequency")

ggplot(head(freq.tri.en, 7), aes(reorder(Ngram, -freq), freq, fill=freq)) +
  geom_bar(stat="identity") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggtitle("Most common EN threegrams") + xlab("Word") + ylab("Frequency")

ggplot(head(freq.bi.ru, 10), aes(reorder(Ngram, -freq), freq, fill=freq)) +
  geom_bar(stat="identity") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggtitle("Most common RU bigrams") + xlab("Word") + ylab("Frequency")

ggplot(head(freq.tri.ru, 7), aes(reorder(Ngram, -freq), freq, fill=freq)) +
  geom_bar(stat="identity") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggtitle("Most common RU threegrams") + xlab("Word") + ylab("Frequency")

```
# Q3. How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%

| Language        | 50% coverage | 90% coverage | Number of words in corpus|
| --------------- | ------------ | ------------ | ------------------------ |
| English         |  362 (12%)   | 1716 (56%)   | 2524                     |
| Russian         |  511 (22%)   | 1994 (84%)   | 2653                     |


```{r, echo=FALSE, word_frequency_coverage, cache=TRUE, warning=FALSE, message=FALSE}
df.en <- calcCoverage(freq.en)
coverage50.en <- min(df.en[df.en$value > 50,]$index)
coverage90.en <- min(df.en[df.en$value > 90,]$index)
coverage.en <- c(coverage50.en, coverage50.en*100/sum(freq.en), coverage90.en, coverage90.en*100/sum(freq.en))
names(coverage.en) <- c("coverage 50%", "percentage 50%", "coverage 90%", "percentage 90%")

par(mfrow=c(1,2))
plot(df.en, type="l",lwd=5, main="EN words frequency coverage", 
     xlab="word index", ylab="frequency coverage",col="darkgrey")
abline(v=coverage50.en, lty=2); abline(h=50, lty=2)
abline(v=coverage90.en, lty=2); abline(h=90,lty=2)
text(x=coverage50.en, y=52, "50% coverage",col="black", adj=c(0,0))
text(x=coverage50.en, y=92, "90% coverage",col="black", adj=c(0,0))

df.ru <- calcCoverage(freq.ru)
coverage50.ru <- min(df.ru[df.ru$value > 50,]$index)
coverage90.ru <- min(df.ru[df.ru$value > 90,]$index)

plot(df.ru, type="l",lwd=5, main="RU words frequency coverage", 
     xlab="word index", ylab="frequency coverage",col="darkgrey")
abline(v=coverage50.ru, lty=2); abline(h=50, lty=2)
abline(v=coverage90.ru, lty=2); abline(h=90,lty=2)
text(x=coverage50.ru, y=52, "50% coverage",col="black", adj=c(0,0))
text(x=coverage50.ru, y=92, "90% coverage",col="black", adj=c(0,0))

# Russian words coverage
coverage.ru <- c(coverage50.ru, coverage50.ru*100/sum(freq.ru), coverage90.ru, coverage90.ru*100/sum(freq.ru))
names(coverage.ru) <- c("coverage 50%", "percentage 50%", "coverage 90%", "percentage 90%")

# Summary
summary <- rbind(round(coverage.en,2),round(coverage.ru,2))
rownames(summary) <- c("english", "russian")
summary

# Have a look at most common words
library(wordcloud)
wordcloud(names(freq.en), freq.en, scale=c(5,0.1), max.words=coverage50.en, random.order=FALSE, 
          rot.per=0.5, use.r.layout=FALSE, colors=brewer.pal(8,"Dark2"))

wordcloud(names(freq.ru), freq.en, scale=c(5,0.1), max.words=coverage50.ru, random.order=FALSE, 
          rot.per=0.5, use.r.layout=FALSE, colors=brewer.pal(8,"Dark2"))

```

# Q4. How do you evaluate how many of the words come from foreign languages? 

We can evaluate the foreign words using POS (Part of Speech) tags using the NLP package.

# Q5. Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases?



