---
title: 'Data Science Capstone Project Milestone Report'
author: "Yevgeny V. Yorkhov"
date: "March 16, 2016"
output: html_document
---

# Synopsis

This is a report of Coursera Data Science Capstone project assignment. Around the world people spend lot's of time on their mobile devices typing words, phrases and sentences. Making typing easier is a pretty good task for mobile developers. The cornerstone of this task is predictive text models. In the Capstone Project we work on understanding and developing such models.

The report shows the exploratory analysis of the data from a corpus called HC Corpora [www.corpora.heliohost.org](http://www.corpora.heliohost.org). 

# Dataset

Dataset is available at the [following URL](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip). It consist of texts downloaded from different Internet websites, divided by 4-languages (English, Russian, German, Finnish) and parted into three files according to the text's source:


- texts from blogs (i.e. en_US.blogs.txt)
- texts from news (i.e. en_US.news.txt)
- texts from twitter (i.e. en_US.twitter.txt)

Also the readme of the dataset is available [here](http://www.corpora.heliohost.org/aboutcorpus.html )

In spite of the texts are language-filtered, but they may contain foreign text. Also the texts may contain some offensive words or phrases that shouldn't be used in the predictive text modeling.

# Basic statistics

Before cleaning the data check basic statistics about full dataset. The table below show the summary:

| Filename             | Size | Number of lines | Number of words |
| -------------------- | ---- | --------------- | --------------- |
| en_US.blogs.txt      | 201M |  899288         | 38222304        |
| en_US.news.txt       | 197M |  1010242        | 35710849        |
| en_US.twitter.txt    | 160M |  2360148        | 30433509        |
| ru_RU.blogs.txt      | 112M |  337100         | 9434050         |
| ru_RU.news.txt       | 114M |  196360         | 9125006         |
| ru_RU.twitter.txt    | 101M |  881414         | 9084961         |


```{r, basicStats, cache=TRUE}
getBasicStats <- function(x) {
  bytesMB <- 1024 * 1024
  conn <- file(x,open="r")
  raw <- readLines(conn)
  close(conn)
  
  res <- file.info(x)$size / bytesMB
  res <- c(res, length(raw))
  res <- c(res, sum(sapply(gregexpr("\\W+", raw), length)))

  names(res) <- c("file size", "number of lines", "number of words")  

  return(res)
}
```

# Data preprocessing

The data should be cleaned before modeling. As far as we are going to predict words we should remove numbers, punctuation, URLs, profanity words and stopwords (that came from grammar) from texts. In addition we should think about quality of sampling, because the dataset is large enough.

```{r, preprocessing, cache=TRUE, warning=FALSE}
getCorpus <- function(filename, sampleSize, profanityWords) {
  conn <- file(filename,open="r")
  
  lines <- readLines(conn)
  if (sampleSize != 0) {
    # Sample data with uniform distribution
    rowNums <- round(runif(sampleSize, min=1, max=length(lines)),0)
    raw <- c(lines[1])
    for(i in rowNums) {
      raw <- c(raw, lines[i])
    }
    rm(lines)
  } else
    raw <- lines
  close(conn)

  # remove punctuation, numbers and tolower the content
  raw <- gsub("[^[:alnum:][:space:]']", ' ', raw)
  raw <- gsub('[[:digit:]]+', ' ', raw)
  raw <- tolower(raw)
  
  # make a corpus
  txt <- VectorSource(raw)
  rm(raw)
  txt.corpus <- Corpus(txt)
  rm(txt)
  
  # Clean the corpus
  txt.corpus <- tm_map(txt.corpus, removeWords, stopwords("english"))
  txt.corpus <- tm_map(txt.corpus, removeWords, stopwords("russian"))
  
  txt.corpus <- tm_map(txt.corpus, removeWords, profanityWords)
  txt.corpus <- tm_map(txt.corpus, stripWhitespace)
  
  return(txt.corpus)
}

blog.en="data/final/en_US/en_US.blogs.txt"
news.en="data/final/en_US/en_US.news.txt"
twitter.en="data/final/en_US/en_US.twitter.txt"
blog.ru="data/final/ru_RU/ru_RU.blogs.txt"
news.ru="data/final/ru_RU/ru_RU.news.txt"
twitter.ru="data/final/ru_RU/ru_RU.twitter.txt"

profanityWords <- read.csv(url("http://www.bannedwordlist.com/lists/swearWords.csv"))

txt.en.blog <- getCorpus(blog.en, 10000, profanityWords)
txt.en.news <- getCorpus(news.en, 10000, profanityWords)
txt.en.twit <- getCorpus(twitter.en, 10000, profanityWords)
txt.ru.blog <- getCorpus(blog.ru, 10000, profanityWords)
txt.ru.news <- getCorpus(news.ru, 10000, profanityWords)
txt.ru.twit <- getCorpus(twitter.ru, 10000, profanityWords)

```

# Question #1 
## Some words are more frequent than others - what are the distribution of word frequencies?

```{r, words_frequency, cache=TRUE}
library(slam)
getFrequency <- function(x) {
  tdm <- TermDocumentMatrix(x)
  tdm.999 <- removeSparseTerms(tdm, sparse = 0.999)
  rm(tdm)
 
  freq <- sort(row_sums(tdm.999), decreasing = TRUE)
  return(freq)
}

calcCoverage <- function(x) {
  df <- data.frame(matrix(nrow=0,ncol=2))
  names(df) <- c("index", "value")
  df[1,] <- c(1, x[1])
  for (i in 2:length(x)) {
    df <- rbind(df, c(i, df$value[i-1] + x[i]))
  }
 
  df$value <- df$value/df$value[i] 
  
  return(df)
}

freq.en.blog <- getFrequency(txt.en.blog)
freq.en.news <- getFrequency(txt.en.news)
freq.en.twit <- getFrequency(txt.en.twit)

freq.ru.blog <- getFrequency(txt.en.blog)
df <- calcCoverage(freq.en.blog)
```


